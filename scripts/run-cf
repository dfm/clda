#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import (division, print_function, absolute_import,
                        unicode_literals)

import os
import sys
import time
import sqlite3
import argparse
import numpy as np
import cPickle as pickle
from multiprocessing import Pool
from collections import defaultdict

np.random.seed(1000005)

try:
    import clda
    clda = clda
except ImportError:
    sys.path.insert(0, os.path.dirname(os.path.dirname(
        os.path.abspath(__file__))))
    import clda
    clda = clda

from clda.icf import ICF
from clda.lda import dirichlet_expectation
from clda.utils import _function_wrapper

parser = argparse.ArgumentParser(description="Run ICF on CitULike data")
parser.add_argument("outdir", help="The results directory")
parser.add_argument("--data", default="data", help="The data directory")
parser.add_argument("-k", "--ntopics", default=200, type=int,
                    help="The number of topics")
parser.add_argument("--mn-doc", default=5, type=int,
                    help="The number of documents required for a user")
parser.add_argument("-f", "--folds", default=5, type=int,
                    help="The number of cross-validation folds")
parser.add_argument("--alpha", default=100.0, type=float,
                    help="The confidence scaling factor")
parser.add_argument("--l2u", default=2.0, type=float,
                    help="The L2 strength for the user vectors")
parser.add_argument("--l2v", default=2.0, type=float,
                    help="The L2 strength for the document vectors")
parser.add_argument("--lda",
                    help="Path to a pre-trained LDA model")
parser.add_argument("--baseline", action="store_true",
                    help="Compute the baseline tf-idf recall")

if __name__ == "__main__":
    pool = Pool()
    args = parser.parse_args()
    try:
        os.makedirs(args.outdir)
    except os.error:
        print("Output directory exists.")

    # Load the dataset.
    with sqlite3.connect(os.path.join(args.data, "abstracts.db")) as conn:
        c = conn.cursor()
        c.execute("""SELECT user_id, arxiv_id,
                        (SELECT arxiv_id FROM articles
                         WHERE id=citeulike.arxiv_id)
                     FROM citeulike""")
        data = c.fetchall()

        c.execute("SELECT user_id, count(*) FROM citeulike GROUP BY user_id")
        user_counts = c.fetchall()

    # Remove any articles that don't have an arxiv id. How the eff did that
    # happen?
    data = filter(lambda d: d[2] is not None, data)

    # Filter to only include users with at least the minimum number of docs.
    user_counts = dict(filter(lambda u: u[1] > args.mn_doc, user_counts))
    data = filter(lambda u: u[0] in user_counts, data)
    np.random.shuffle(data)

    # Calculate the document counts.
    doc_counts = defaultdict(int)
    for u, d, aid in data:
        doc_counts[d] += 1

    # Map the users and documents to integers.
    user_map = dict(zip(user_counts.keys(), range(len(user_counts))))
    doc_map = dict(zip(doc_counts.keys(), range(len(doc_counts))))

    # Extract the arxiv ids.
    arxiv_ids = [None for i in range(len(doc_map))]
    for u, d, aid in data:
        arxiv_ids[doc_map[d]] = aid

    if args.baseline:
        print("Computing word counts")
        corpus_counts = defaultdict(int)
        doc_word_counts = defaultdict(lambda: defaultdict(int))
        with sqlite3.connect(os.path.join(args.data, "abstracts.db")) as conn:
            c = conn.cursor()
            for d in doc_map:
                c.execute("SELECT * FROM articles WHERE rowid=?", (d, ))
                doc = c.fetchone()
                dc = doc_word_counts[doc_map[d]]
                for w in doc[3].split() + doc[4].split():
                    dc[w] += 1
                for w in dc:
                    corpus_counts[w] += 1

        print("Normalizing word counts")
        D = len(doc_word_counts)
        for d, dwc in doc_word_counts.iteritems():
            for w, dwcw in dwc.iteritems():
                dwcw *= np.log(D) - np.log(corpus_counts[w])

    # Convert the data to integers.
    data = [(user_map[u], doc_map[d], d) for u, d, aid in data]

    # Save the user and document maps to the results directory.
    pickle.dump((user_map, doc_map),
                open(os.path.join(args.outdir, "maps.pkl"), "wb"), -1)

    # Print the dataset stats.
    print("{0} unique users".format(len(user_counts)))
    print("{0} unique documents".format(len(doc_counts)))
    print("{0} user-document pairs".format(len(data)))

    # Load the LDA model and run inference on the documents.
    ntopics = args.ntopics
    if args.lda is not None:
        print("Running LDA inference")
        lda = pickle.load(open(args.lda))
        reader = pickle.load(open(os.path.join(os.path.dirname(
            os.path.abspath(args.lda)), "reader.pkl")))

        documents = map(lambda i: reader.parse_document(reader[i]), arxiv_ids)

        ntopics = lda.ntopics
        theta = np.empty((len(doc_counts), ntopics))
        gammas = pool.map(_function_wrapper(lda, "infer"), documents)
        gammas = np.array(gammas)
        thetas = np.exp(dirichlet_expectation(gammas))

    # Build the CV folds.
    training_set = []
    cv_folds = [[] for i in range(args.folds)]
    cv_counts = defaultdict(int)
    for u, d, name in data:
        if doc_counts[name] < args.folds:
            # Ensure that all the documents are in the training set once.
            training_set.append((u, d))
        else:
            if cv_counts[d] < args.folds:
                # Split the first instances of the document into the CV folds.
                ind = cv_counts[d]
            else:
                # Randomly assign the following occurrences to the folds and
                # training set.
                ind = np.random.randint(args.folds)
            cv_folds[ind].append((u, d))
        cv_counts[d] += 1

    # Print the train and test set stats.
    print("{0} entries in the training set".format(len(training_set)))
    print("{0} folds with {1} entries".format(len(cv_folds),
                                              [len(f) for f in cv_folds]))

    # Concatenate the training set and some of the held-out sets.
    t = training_set
    for s in cv_folds[:-1]:
        t += s

    if args.baseline:
        print("Training baseline model")
        user_word_counts = defaultdict(dict)
        user_training_set = defaultdict(list)
        for u, d in t:
            user_word_counts[u] = dict(user_word_counts[u],
                                       **(doc_word_counts[d]))
            user_training_set[u].append(d)

        # Compute the test set.
        test_set = defaultdict(list)
        for u, d in cv_folds[-1]:
            test_set[u].append(d)

        print("Computing tf-idf recall")
        scores = np.empty(len(doc_map))
        recall_ns = range(50, 501, 50)
        full_recall = np.zeros(len(recall_ns))
        for u, tu in test_set.iteritems():
            vec = user_word_counts[u]

            for d in doc_map.values():
                if d in user_training_set[u]:
                    scores[d] = 0.0
                    continue
                score, norm = 0.0, 0.0
                for w, c in doc_word_counts[d].iteritems():
                    score += vec.get(w, 0.0) * c
                    norm += c*c
                if norm > 0.0:
                    scores[d] = score / np.sqrt(norm)
                else:
                    scores[d] = 0.0

            # Compute recommendations.
            inds = np.argsort(scores)[::-1]
            recall = np.cumsum([i in tu
                                for i in inds[:max(recall_ns)+1]])/len(tu)
            full_recall += recall[recall_ns]
        print(full_recall / len(test_set))
        assert 0

    # Build the ICF model.
    if args.lda is None:
        model = ICF(ntopics, len(user_counts), len(doc_counts),
                    alpha=args.alpha, l2v=args.l2v, l2u=args.l2u)
    else:
        model = ICF(ntopics, len(user_counts), len(doc_counts),
                    alpha=args.alpha, l2v=args.l2v, l2u=args.l2u,
                    theta=thetas)

    # Estimate the random recall rate.
    print("Random recall")
    rand_recall = model.mean_recall([], cv_folds[0], pool=pool)
    (open(os.path.join(args.outdir, "random-recall.txt"), "w")
     .write(" ".join(map("{0}".format, rand_recall))))

    # Initialize the results file.
    fn = os.path.join(args.outdir, "cf-convergence.txt")
    open(fn, "w").close()

    # Train the model.
    strt = time.time()
    for i, recall in enumerate(model.train(t, test_set=cv_folds[-1],
                                           pool=pool,
                                           lda=args.lda is not None)):
        if args.lda is not None and i == 0:
            (open(os.path.join(args.outdir, "lda-recall.txt"), "w")
             .write(" ".join(map("{0}".format, model.lda_recall))))

        print("Iteration {0}: held-out recall = {1}".format(i, recall))
        with open(fn, "a") as f:
            f.write("{0:d} {1:e} {2}\n".format(i, time.time()-strt,
                                               " ".join(map("{0}".format,
                                                            recall))))
        pickle.dump(model,
                    open(os.path.join(args.outdir,
                         "cf-model.{0:04d}.pkl".format(i)), "wb"), -1)
