\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{fancyhdr}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}

\usepackage{listings}
\usepackage{color}
\lstset{%
        basicstyle=\footnotesize\ttfamily,
        showspaces=false,
        showstringspaces=false,
        tabsize=2,
        breaklines=false,
        breakatwhitespace=true,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
    }

\usepackage{graphicx}

% header
\fancyhead{}
\fancyfoot{}
\fancyfoot[C]{\thepage}
\fancyhead[R]{Daniel Foreman-Mackey}
\fancyhead[L]{Statistical Natural Language Processing --- Project Proposal}
\pagestyle{fancy}
\setlength{\headsep}{10pt}
\setlength{\headheight}{20pt}

% shortcuts
\newcommand{\Eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}

\newcommand{\etal}{\emph{et al.}}

\newcommand{\pr}[1]{\ensuremath{p\left (#1 \right )}}
\newcommand{\lk}[1]{\ensuremath{\mathcal{L} \left ( #1 \right )}}
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\dd}{\ensuremath{\, \mathrm{d}}}
\newcommand{\normal}[2]{\ensuremath{\mathcal{N} \left ( #1; #2 \right ) }}
\newcommand{\T}{^\mathrm{T}}

\newcommand{\data}{\mathcal{D}}
\newcommand{\code}[1]{{\sffamily #1}}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

I'm very interested in the question of discovery and recommendation in the
scientific literature.
In particular, the ArXiv\footnote{\url{http://arxiv.org}} offers an incredible
wealth of data for NLP research and this dataset has been \emph{complete} in
astrophysics---my field of interest---since the mid-1990s.
With this in mind, I have a set of problems in this vein that I aim to
complete for this project.
The final model that I would like to implement is an extension of the
\emph{collaborative topic modeling} model proposed by \citet{blei}.
That paper combines a standard topic model with a matrix factorization model
of collaborative filtering in order to explicitly consider user activity when
training and testing.
This is \emph{much more interesting} to me than standard topic modeling
because it provides a more relevant metric to measure performance:
the predictive power for \emph{real user behavior}!
It seems to be standard practice in the topic modeling business to measure
model performance by simply showing that the perplexity improves and showing a
few of the common words in each discovered topic.
This is interesting if all you want to do is qualitatively explore the
distribution of topics in your corpus but it doesn't seem particularly
relevant as a recommendation quality metric.

The datasets that I have available for this project are:
\begin{itemize}
\item{the authors, abstract, title, publication date and categories for
      every paper posted to the
      ArXiv\footnote{\url{http://arxiv.org/help/oa/index}},}
\item{the full text (in \LaTeX or PDF format) for most of the above
      articles\footnote{\url{http://arxiv.org/help/bulk_data_s3}}, and}
\item{the complete set of user bibliographies from
      CiteULike\footnote{\url{http://www.citeulike.org/faq/data.adp}}.}
\end{itemize}
As of 2013-10-28, in the CiteULike dataset, there are 122,638 unique articles
in 8,598 user libraries with a total of 384,381 user-article pairs.
There are approximately 800k articles in the full ArXiv dataset.

Using these data, the first model that I would like to implement is an
efficient \emph{online variational LDA} \citep{ovlda} and compare the
performance of this model on the full text articles to the performance on the
abstracts and titles alone.
This is a very interesting model for a dataset of this size where a single
bulk pass through the dataset can be quite expensive.
In parallel, I would like to develop a probabilistic matrix factorization
model \citep{pmf} for collaborative filtering using the CiteULike data only.
Finally, I aim to combine these two models into a collaborative topic model
and then derive and implement the online variational inference updates for
this model \citep[following][]{ov}.

For evaluation of the performance of my models, I'll primarily use the
recall of predictions for user libraries but I'll also consider the perplexity
of the model and the final distribution of topics.

I think that the results of these experiments will be interesting and novel
for several reasons.
Firstly, I haven't been able to find any literature that builds topic models
using the full text of scientific articles.
My intuition is that this will provide the data necessary to constrain a more
fine grained model that will be able to detect topics---specific data analysis
techniques and tools, for example---that don't often make it into an abstract.
Also, \citet{blei} only used the abstracts and titles for documents \emph{in
their social dataset} and concluded that the topic model didn't provide
substantial gains over collaborative filtering techniques (except when
considering new articles).
In my model, there will be many more documents in the corpus used for training
so I expect the topic model to be much more instructive.
Finally, the learning procedure implemented by \citet{blei} is, as far as I
can tell, pure hack.
It doesn't seem challenging to derive the correct variational EM procedure
which can then be applied stochastically for better scaling to large datasets.

\begin{thebibliography}{}\raggedright

\bibitem[Hoffman, Blei \& Bach(2011)]{ovlda}
M.~Hoffman, D.~Blei, \& F.~Bach. \textbf{Online learning for latent Dirichlet
allocation}, Neural Information Processing Systems, 2010

\bibitem[Hoffman \etal(2013)]{ov}
M.~Hoffman, D.~Blei, J.~Paisley, \& C.~Wang. \textbf{Stochastic variational
inference}. Journal of Machine Learning Research, 2013

\bibitem[Salakhutdinov \& Mnih(2008)]{pmf}
R.~Salakhutdinov \& A.~Mnih. \textbf{Probabilistic matrix factorization},
Advances in Neural Information Processing Systems, 2008

\bibitem[Wang \& Blei(2011)]{blei}
C.~Wang \& D.~M.~Blei. \textbf{Collaborative Topic Modeling for
Recommending ScientiÔ¨Åc Articles}, Knowledge Discovery and Data Mining, 2011

\end{thebibliography}

\end{document}
