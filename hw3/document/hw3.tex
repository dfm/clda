\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{fancyhdr}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}

\usepackage{listings}
\usepackage{color}
\lstset{language=Python,
        basicstyle=\footnotesize\ttfamily,
        showspaces=false,
        showstringspaces=false,
        tabsize=2,
        breaklines=false,
        breakatwhitespace=true,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
    }

\usepackage{graphicx}

% header
\fancyhead{}
\fancyfoot{}
\fancyfoot[C]{\thepage}
\fancyhead[R]{Daniel Foreman-Mackey}
\fancyhead[L]{Statistical Natural Language Processing --- Homework 3}
\pagestyle{fancy}
\setlength{\headsep}{10pt}
\setlength{\headheight}{20pt}

% shortcuts
\newcommand{\Eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}

\newcommand{\pr}[1]{\ensuremath{p\left (#1 \right )}}
\newcommand{\lk}[1]{\ensuremath{\mathcal{L} \left ( #1 \right )}}
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\dd}{\ensuremath{\, \mathrm{d}}}
\newcommand{\normal}[2]{\ensuremath{\mathcal{N} \left ( #1; #2 \right ) }}
\newcommand{\T}{^\mathrm{T}}

\newcommand{\data}{\mathcal{D}}
\newcommand{\code}[1]{{\sffamily #1}}


\begin{document}

As with the last assignment, I decided to implement my homework in Python
instead of using the provided Java code.
I was surprised to find that the language modeling implemented in pure-Python
was very computationally efficient.
This is because the data structures in the standard library are pretty
impressive!
I also started implementing a version that looked a bit more like the provided
code (using a lot of objects) and that was painfully slow.
I wasn't able to make the Viterbi algorithm bearable in pure-Python so I
dropped down to C for that part (but I still evaluate the trigram/emission
model using Python).
All of the code that I used for this assignment is available on GitHub:
\url{https://github.com/dfm/nlp}.

\section{The Model}

The problem that we're trying to solve in this assignment is: given a
sentence $\bvec{w} = \{w_1, w_2, \ldots, w_n\}$, what is the part of
speech $t_i$ associated with each word $w_i$.
The model that we're considering is the simple hidden Markov model (HMM) shown
in \fig{pgm}\footnote{This graphical model figure was generated using my
Python module called Daft \url{http://daft-pgm.org} $<$/advertisement$>$}.
This model relies on two generative probability distributions
\begin{eqnarray}
& p(t_i \,|\, t_{i-2},\,t_{i-1}) & \quad \mathrm{transitions} \\
& p(w_i \,|\, t_i) & \quad \mathrm{emissions}
\end{eqnarray}
These are the relevant quantities because we are looking to find the tag
sequence $\bvec{t}$ that maximizes the posterior probability distribution
\begin{eqnarray}
p(\bvec{t}\,|\,\bvec{w}) &=& \frac{p(\bvec{t},\,\bvec{w})}{p(\bvec{w})}
                             \nonumber\\
                         &\propto& p(\bvec{t},\,\bvec{w}) \nonumber\\
                         &=& \prod_i p(t_i \,|\, t_{i-2},\,t_{i-1}) \,
                                     p(w_i \,|\, t_i) \quad.
\end{eqnarray}
The following sections describe the different methods for estimating these
distributions and the results of more sophisticated models.

\begin{figure}[htbp]
\begin{center}
    \includegraphics{pgm.pdf}
\end{center}
\caption{%
The graphical model describing the probabilistic generative model used in this
assignment.
The variable $t_k$ is the part-of-speech tag associated with the observed
word $w_k$.
There are two tags in each latent node and one is shared with the previous
node because the model that I will implement considers the local trigram
context when inferring the tags.
The initial $\left <s,s\right >$ and final states $\left < e,e \right >$
explicitly mark the beginning and end of the sentence.
\figlabel{pgm}}
\end{figure}

\section{The Baseline Model}

The first model that I built was similar to the \emph{Baseline} model in the
provided code but with a slightly different model of unknown word
probabilities (with similar performance).
In this model, the tag context is ignored and instead, the tag probabilities
are given by the function (estimated from the training data)
\begin{eqnarray}\eqlabel{cond-emission}
p(t_i\,|\,w_i) &=& \frac{f(t_i,w_i)}{f(w_i)}
\end{eqnarray}
where $f(t_i, w_i)$ is a count of the number of times the word $w_i$ appears
with the part-of-speech tag $t_i$ in the training set.
Similarly, $f(w_i)$ is the total number of times the word $w_i$ appears in the
training examples.
This simple model clearly breaks down for words that have never been seen
giving exactly zero probability to each tag.

\paragraph{Unknown words}
The provided code deals with this by building a second distribution
\begin{eqnarray}
p(t_i\,|\,\mathrm{unk})
\end{eqnarray}
that is estimated from the training data by looping through the training data
and counting the number of times the tag $t_k$ is associated with the
\emph{first appearance} of the word $w_n$.
This seems like a reasonable model but for consistency with my better unknown
word model, I made a simpler model
\begin{eqnarray}
p(t_i\,|\,\mathrm{unk}) &=& \frac{\left . f(t_i)\right |_\theta}{N_\theta}
\end{eqnarray}
where $\left.f(t_i)\right|_\theta$ is the number of times the tag $t_i$
appears in a subset of the training data $\theta$ and $N_\theta$ is the number
of examples in the same subset.
I chose the ``unknown proxy'' subset by counting the number of times each word
appears in the full training corpus and extracting all the examples that emit
words that appear fewer than $K$ times.
This adds a free parameter to the model but I found that the performance
wasn't particularly sensitive to the choice for $1 \lesssim K \lesssim 100$
(I'll come back to this later in mode detail).
This model does seem to do surprisingly well but there's a lot we can do to
make it better.

\section{Trigram Transitions}

The first step in building a better model is to take the local context into
account.
In particular, we'll condition on the two previous tags to compute
the transition probabilities.
Following \citet{tnt}, I estimated the transition probabilities using the
linearly interpolated form
\begin{eqnarray}\eqlabel{trigram}
p(t_i\,|\, t_{i-2},\,t_{i-1}) & = &
    (1-\lambda_2-\lambda_3)\,\hat{p}(t_i) +
    \lambda_2\,\hat{p}(t_i\,|\,t_{i-1}) +
    \lambda_3\,\hat{p}(t_i\,|\,t_{i-2},\,t_{i-1})
\end{eqnarray}
where
\begin{eqnarray}
\hat{p}(t_i) &=& \frac{f(t_i)}{N} \\
\hat{p}(t_i\,|\,t_{i-1}) &=& \frac{f(t_{i-1},t_i)}{f(t_{i-1})} \\
\hat{p}(t_i\,|\,t_{i-2},\,t_{i-1}) &=&
    \frac{f(t_{i-2},t_{i-1},t_i)}{f(t_{i-2},t_{i-1})}
\end{eqnarray}
and $N$ is the total number of tag trigrams in the training set and $f(\cdot)$
is the count of the number of times the given tag sequence is observed in
the training data.
This model adds two more free parameters that need to be chosen properly.
My preferred method is to determine these values using cross-validation
but I also implemented the \emph{deleted interpolation method} as described
by \citet{tnt}.
Using this method, I estimated these tuning parameters to be:
\begin{eqnarray}
    \lambda_2 &=& 0.2778 \quad \mathrm{and} \nonumber\\
    \lambda_3 &=& 0.5709 \quad.
\end{eqnarray}
I'll discuss my final choices for these parameters in the results section.

\section{A Better Model of Emission Probabilities}

For my model of the emission probability distribution $p(w_i\,|\,t_i)$, I
again inferred an empirical distribution based directly on the frequencies in
the training set.
As a result, I estimated this distribution as
\begin{eqnarray}\eqlabel{emission}
p(w_i\,|\,t_i) = \frac{f(t_i,w_i)}{f(t_i)}\quad.
\end{eqnarray}
This is similar to \eq{cond-emission} but instead of estimating the tag
probabilities conditional on the observed word, I directly estimated the
likelihood function.

\section{Unknown Words Revisited}

The model in \eq{emission} will still give a model with any new words a
probability of exactly zero so this is clearly not acceptable.
To deal with this, I also built a more sophisticated feature-based unknown
word model.
The model that I settled on was a very simple---fast to train and
evaluate---empirical model based on a set of engineered features.
I expect that a probabilistic discriminative classifier like the Maximum
Entropy model that we implemented in the previous assignment would offer
somewhat better performance at the cost of a much more computationally
expensive training period.
My empirical model trains quickly with a single pass through the training data
and the evaluation cost also somewhat lower than the MaxEnt model.
Despite all this, the performance on unknown words is (as you will see below)
very competitive after a bit of tuning.

The features that I chose to use are:
\begin{itemize}
\item{a smoothed suffix distribution,}
\item{the case of the first character, and}
\item{an indication of the existence of numbers in the word.}
\end{itemize}

The smoothed suffix distribution looks similar to \eq{trigram} above but for
the final $N$ characters of the word.
I chose to use up to 3 letters because this keeps the model relatively small
while still offering a lot of flexibility.
In this case, the estimate of the unknown word suffix probability
distribution (to be trained on the subset of the training data discussed
above) is the following
\begin{eqnarray}\eqlabel{psuff}
p_\mathrm{suff}(w_i\,|\,t_i) &=& \theta_0\frac{f(t_i)}{N_\theta} +
    \frac{\theta_1\,f(t_i,\,\mathrm{suff}_1[w_i])
          + \theta_2\,f(t_i,\,\mathrm{suff}_2[w_i])
          + \theta_3\,f(t_i,\,\mathrm{suff}_3[w_i])}{f(t_i)}
\end{eqnarray}
where $\theta_0 = 1 - \theta_1 - \theta_2 - \theta_3$ and
$\mathrm{suff}_m[w_i]$ indicates the last $m$ letters of the word $w_i$.

I also built an empirical probability distribution for capitalization
conditioned on the tags based on the same training subset.
\begin{eqnarray}\eqlabel{pcap}
p_\mathrm{cap} (w_i\,|\,t_i) &=&
    \frac{f(t_i,\,\mathrm{cap}[w_i])+1}{N_\theta+1}
\end{eqnarray}
where the $\mathrm{cap}$ function is defined as
\begin{eqnarray}
\mathrm{cap}(w_i) &=& \left \{ \begin{array}{ll}
    0 & \mbox{if the first letter of $w_i$ is lowercase,} \\
    1 & \mbox{if the first letter of $w_i$ is uppercase.} \\
\end{array}\right .
\end{eqnarray}
The added one in \eq{pcap} is there as a smoothing term to make sure that this
probability never goes to exactly zero.

The final feature of my unknown word model is a count of the number of digits
in the word.
I tried just using a flag indicating the existence of (at least) one number in
the word but I found that a richer structure provided a somewhat better
out-of-domain validation set performance.
In the end, the best results that I found resulted from a model that I built
by estimating a probability of observing a word with $K$ digits for each tag
\begin{eqnarray}\eqlabel{pnum}
p_\mathrm{num}(w_i\,|\,t_i) &=&
    \frac{f(t_i,\,\mathrm{digits}[w_i])+1}{f(t_i)+K}
\end{eqnarray}
where
\begin{eqnarray}
\mathrm{digits}[w_i] &=& \left \{ \begin{array}{ll}
    0 & \mbox{if there are no digits in $w_i$,} \\
    1 & \mbox{if there is one digit in $w_i$,} \\
    \cdots & \\
    K & \mbox{if there are $\ge K$ digits in $w_i$.}
\end{array}\right.
\end{eqnarray}
Note that \eq{pnum} is smoothed in the same way as \eq{pcap}.

Combining the above features, we get the full probability distribution for
unknown words
\begin{eqnarray}\eqlabel{pfull}
p(w_i\,|\,t_i) &=& p_\mathrm{suff}(w_i\,|\,t_i) \,
                   p_\mathrm{cap}(w_i\,|\,t_i) \,
                   p_\mathrm{num}(w_i\,|\,t_i) \quad.
\end{eqnarray}
Because of the smoothing applied to each term in this product, this
probability should never be exactly zero.
This is necessary because the features are not a perfect set of summary
statistics and in practice some validation examples end up getting zero
probability for \emph{every tag sequence}.
This would make inference impossible and it is clearly an artificial effect
since we are explicitly discussing \emph{unknown words}.

\paragraph{Rejected features}
I considered various other features that I ended up rejecting because they
made no effect on the validation performance (or sometimes even made the
results worse).
One feature that I considered was the existence of \emph{punctuation} in the
word.
My expectation was that unrecognized punctuation would be better classified by
including this feature.
Instead, most punctuation errors seem to be related to differences between the
``gold'' tag rules of the in-domain and out-of-domain datasets.
I'll discuss this fact in more depth below.

\section{Inference}

The previous sections describe the form of the probabilistic generative model
that we'll assume for this problem.
They also explain the training procedure given a set of tagged sentences.
Given this trained model, the only step left is inference: finding the tag
sequence that maximizes the posterior probability.
The baseline model in the provided code implements a greedy inference
procedure.
This is a very computationally efficient technique because it only requires a
single pass through the sentence and at each word, it only requires the
evaluation of a single set of conditional probabilities.
The problem is that it doesn't have any guarantee of finding the true maximum
probability path.

\paragraph{Viterbi algorithm}
For the HMM model described by \fig{pgm}, it is possible---and relatively
efficient---to compute the exact maximum \emph{a posteriori} path using the
Viterbi method.
Qualitatively, this method works by iterating through the words in the
sentence and at each word $w_i$, evaluating and saving---for each tag
$t_i$---the preceding path of tags $\bvec{t}_{<i}$ that gives the maximum
probability up to the word $i$.
It's easy to see that by the time you've iterated through the entire sentence,
the maximum probability tag will exactly define the best path.
This method is no longer exponential---like the na\"ive method---but it is
still relatively computationally expensive.
One optimization that I implemented is to explicitly exclude any zero
probability paths so that you don't have to compute the probabilities
conditioned on those paths.
In this case, this helps \emph{a lot} because many words cannot possibly be
generated by a given tag.
Another optimization that would probably be very useful in a real world
application---but I didn't implement it this time---would be to prune the tree
with some relative probability threshold at each step.
In other words, exclude any tags that give a probability less than some
$\theta < 1$ times the maximum probability at that step.
This would be similar to a beam search algorithm but I expect that it would be
more reliable because at uncertain steps---where each possible branch has
close to the same probability---many of the branches would be thrown away by a
beam search whereas this pruned Viterbi method would keep more possibilities
open.

\section{Results}

In the previous section, I developed 4 different ``models'' for the
part-of-speech tagging problem:
\begin{enumerate}
\item{{\bf Baseline}: a context independent scorer using the greedy
      decoder,}
\item{{\bf Trigram}: a trigram scorer using the greedy decoder,}
\item{{\bf Viterbi}: the same trigram scorer using the exact Viterbi decoder,
      and}
\item{{\bf Unknown}: the same trigram scorer with a better model of unknown
      words using the Viterbi decoder.}
\end{enumerate}
The \emph{Baseline} model has one free parameter: the threshold count for
estimating the unknown word distribution.
The \emph{Trigram} and \emph{Viterbi} models have 2 extra parameters: the
$\lambda$ interpolation coefficients used by the trigram scorer.
My specific formulation of the \emph{Unknown} model adds 3 more parameters:
the $\theta$s controlling the interpolation in the suffix model.
Here, I will discuss the effects of each of these parameters on the in-domain
and out-of-domain validation sets.
In general, I found that the performance of all of my methods was
substantially ($\lesssim 2\%$) worse on the test data than on the
out-of-domain validation set but I was still able to achieve competitive
performance on all of the non-training data.

\subsection{The \emph{Baseline} model}

As mentioned previously, the only parameter in the \emph{Baseline} model is
the unknown word threshold and in fact, the performance of the model on the
validation sets isn't very sensitive to this number (as you can see in
table~\ref{tab:baseline}).
For thresholds in the range $1-50$, the achieved is exactly what I would
expect based on the numbers in the homework.
For thresholds of $\gtrsim 100$, the performance on unknown words drops
drastically.
The fact that the unknown word accuracy decreases with increasing threshold
is intuitive because the expectation is that the distribution of \emph{rare}
words should be more representative of the unknown words than the full corpus.
That being said, I don't have any good idea for why this effect seems to be
exhibited as a step function in this specific model.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{c c cccc}
\toprule
&& \multicolumn{2}{c}{in-domain [\%]} & \multicolumn{2}{c}{out-of-domain [\%]}\\
threshold & $N$ & known & unknown & known & unknown \\\midrule
1 & 17556 & 91.669 & 42.710 & 87.363 & 52.944\\
5 & 53551 & 91.669 & 42.710 & 87.363 & 52.944\\
10 & 81217 & 91.669 & 42.710 & 87.363 & 52.944\\
50 & 187735 & 91.669 & 42.710 & 87.363 & 52.944\\
100 & 254225 & 90.888 & 9.651 & 83.834 & 9.898\\
1000 & 511208 & 90.888 & 9.651 & 83.834 & 9.898\\
10000 & 736952 & 90.888 & 9.651 & 83.834 & 9.898\\\bottomrule
\end{tabular}
\end{center}
\caption{%
The performance of the \emph{Baseline} model (described in the text) on the
in-domain and out-of-domain validation sets.
The column ``threshold'' indicates the maximum number of times a word can
appear in the training corpus and still be used to train the unknown word
model and $N$ is the number of examples that meet this criterion.
The performance is measured in percentage of validation examples correctly
classified.
\label{tab:baseline}}
\end{table}

\subsection{The \emph{Trigram} \& \emph{Viterbi} models}

In total, the \emph{Trigram} and \emph{Viterbi} models have 4 parameters each
and since they control the same part of the model in both cases I expected the
model performance to scale similarly with these parameters in both cases.
The check the performance, I ran a small grid in threshold and the smoothing
parameters.
For the range of the smoothing parameters, I chose values close to the ones
estimated using the deleted interpolation technique described above.
Table~\ref{tab:trigram} shows the performance of the \emph{Trigram} model on
the two validation sets and (again) the performance is not very sensitive to
these parameters.
The best model in this case has:
\begin{eqnarray}
\mathrm{threshold} = 2,\quad\lambda_2 = 0.2\quad\mbox{and}\quad
\lambda_3 = 0.6\quad.
\end{eqnarray}
Similarly, for the \emph{Viterbi} model, the results are shown in
table~\ref{tab:viterbi}.
Much of the scaling in accuracy is the same as in the \emph{Trigram} case but
the preferred model is slightly different:
\begin{eqnarray}
\mathrm{threshold} = 2,\quad\lambda_2 = 0.2\quad\mbox{and}\quad
\lambda_3 = 0.7\quad.
\end{eqnarray}
It is interesting to note that the unknown word accuracy is
\emph{significantly} better when doing exact inference.
This is because the unknown word model that I'm using in these baseline models
is very noisy.
This means that when the greedy decoder hits an unknown word, the signal is
muddied by the noise in the unknown word model even if the true optimum is
well constrained by the tag sequence probabilities.

\begin{table}[phtb]
\begin{center}
\begin{tabular}{c cc cccc}
\toprule
&&& \multicolumn{2}{c}{in-domain [\%]} & \multicolumn{2}{c}{out-of-domain [\%]}\\
threshold & $\lambda_2$ & $\lambda_3$ & known & unknown & known & unknown \\\midrule
2 & 0.2 & 0.5 & 93.685 & 51.951 & 89.016 & 45.228\\
{\bf 2} & {\bf 0.2} & {\bf 0.6} & {\bf 93.726} & {\bf 52.156} & {\bf 89.070} &
{\bf 45.584}\\
2 & 0.2 & 0.7 & 93.675 & 52.361 & 89.024 & 45.431\\
2 & 0.3 & 0.5 & 93.702 & 51.848 & 89.032 & 45.228\\
2 & 0.3 & 0.6 & 93.680 & 52.156 & 89.066 & 45.584\\
2 & 0.4 & 0.5 & 93.656 & 52.156 & 88.991 & 45.381\\
5 & 0.2 & 0.5 & 93.661 & 50.821 & 89.016 & 45.228\\
5 & 0.2 & 0.6 & 93.714 & 51.745 & 89.049 & 45.330\\
5 & 0.2 & 0.7 & 93.668 & 52.156 & 89.032 & 45.533\\
5 & 0.3 & 0.5 & 93.697 & 51.643 & 89.032 & 45.228\\
5 & 0.3 & 0.6 & 93.678 & 52.053 & 89.061 & 45.533\\
5 & 0.4 & 0.5 & 93.653 & 52.053 & 88.991 & 45.381\\
10 & 0.2 & 0.5 & 93.656 & 50.821 & 89.024 & 45.279\\
10 & 0.2 & 0.6 & 93.712 & 51.643 & 89.045 & 45.279\\
10 & 0.2 & 0.7 & 93.666 & 52.053 & 89.032 & 45.533\\
10 & 0.3 & 0.5 & 93.700 & 51.745 & 89.036 & 45.279\\
10 & 0.3 & 0.6 & 93.675 & 51.951 & 89.057 & 45.482\\
10 & 0.4 & 0.5 & 93.656 & 52.156 & 88.986 & 45.279\\
\bottomrule
\end{tabular}
\end{center}
\caption{%
The performance of the \emph{Trigram} model (described in the text) on the
in-domain and out-of-domain validation sets.
The column ``threshold'' indicates the maximum number of times a word can
appear in the training corpus and still be used to train the unknown word
model and $\lambda_2$ and $\lambda_3$ are the parameters that control the
interpolation of the trigram scorer.
The performance is measured in percentage of validation examples correctly
classified.
\label{tab:trigram}}
\end{table}

\begin{table}[phtb]
\begin{center}
\begin{tabular}{c cc cccc}
\toprule
&&& \multicolumn{2}{c}{in-domain [\%]} & \multicolumn{2}{c}{out-of-domain [\%]}\\
threshold & $\lambda_2$ & $\lambda_3$ & known & unknown & known & unknown \\\midrule
2 & 0.2 & 0.5 & 95.502 & 59.035 & 91.451 & 53.198\\
2 & 0.2 & 0.6 & 95.572 & 59.446 & 91.542 & 53.858\\
{\bf 2} & {\bf 0.2} & {\bf 0.7} & {\bf 95.652} & {\bf 60.164} & {\bf 91.617} &
{\bf 54.061}\\
2 & 0.3 & 0.5 & 95.592 & 60.267 & 91.496 & 53.299\\
2 & 0.3 & 0.6 & 95.623 & 60.062 & 91.596 & 53.706\\
2 & 0.4 & 0.5 & 95.611 & 60.267 & 91.550 & 53.299\\
5 & 0.2 & 0.5 & 95.478 & 57.906 & 91.384 & 52.437\\
5 & 0.2 & 0.6 & 95.572 & 59.446 & 91.476 & 52.995\\
5 & 0.2 & 0.7 & 95.631 & 59.446 & 91.513 & 52.944\\
5 & 0.3 & 0.5 & 95.553 & 58.932 & 91.438 & 52.538\\
5 & 0.3 & 0.6 & 95.619 & 60.062 & 91.559 & 53.401\\
5 & 0.4 & 0.5 & 95.582 & 59.240 & 91.455 & 52.183\\
10 & 0.2 & 0.5 & 95.446 & 56.674 & 91.334 & 51.777\\
10 & 0.2 & 0.6 & 95.531 & 57.906 & 91.401 & 52.132\\
10 & 0.2 & 0.7 & 95.623 & 59.035 & 91.484 & 52.640\\
10 & 0.3 & 0.5 & 95.517 & 57.084 & 91.392 & 52.030\\
10 & 0.3 & 0.6 & 95.587 & 58.830 & 91.496 & 52.741\\
10 & 0.4 & 0.5 & 95.558 & 58.008 & 91.442 & 51.980\\
\bottomrule
\end{tabular}
\end{center}
\caption{%
The same as table~\ref{tab:trigram} but for the \emph{Viterbi} model.
\label{tab:viterbi}}
\end{table}

\subsection{The \emph{Unknown} model}

The \emph{Unknown} model includes 3 extra parameters on top of the parameters
needed for the previous two models: the $\lambda$s that control the smoothing
of the suffix model for unknown words.
The thought of running a full grid in all of these parameters was a little
daunting so I decided to fix the $\lambda$ parameters at their preferred
values from the previous section ($\lambda_2 = 0.2$ and $\lambda_3 = 0.7$) and
then run a grid of models with different values of the $\theta$s.
The results of this grid are shown in table~\ref{tab:unknown}.
From this table, you'll notice that the preferred values are:
\begin{eqnarray}
\mathrm{threshold} = 5,\quad\theta_1 = 0.1,\quad\theta_2 = 0.5\quad
\mbox{and}\quad \theta_3 = 0.3\quad.
\end{eqnarray}
It's interesting to note that the weight of the $\mathrm{suff}_2$ term is
larger than the $\mathrm{suff}_3$ term.
I think that this is probably true because many common suffixes are only two
letters (for example ``-ly'', ``-ed'', ``-es'', ``-ic'', \emph{etc.}).
Many of the longer suffixes---such as ``-ing'', ``-tion'', ``-ous'',
\emph{etc.}---can also be well classified by their last two letters.

\begin{table}[phtb]
\begin{center}
\begin{tabular}{c ccc cccc}
\toprule
&&&& \multicolumn{2}{c}{in-domain [\%]} & \multicolumn{2}{c}{out-of-domain [\%]}\\
threshold & $\theta_1$ & $\theta_2$ & $\theta_3$ & known & unknown & known & unknown \\\midrule
2 & 0.01 & 0.3 & 0.3 & 96.220 & 82.854 & 92.824 & 67.766\\
2 & 0.01 & 0.3 & 0.5 & 96.225 & 83.162 & 92.986 & 69.695\\
2 & 0.01 & 0.5 & 0.3 & 96.223 & 83.162 & 93.020 & 70.152\\
2 & 0.05 & 0.3 & 0.3 & 96.225 & 83.162 & 92.920 & 68.832\\
2 & 0.05 & 0.3 & 0.5 & 96.232 & 83.573 & 93.086 & 70.964\\
2 & 0.05 & 0.5 & 0.3 & 96.232 & 83.573 & 93.103 & 71.117\\
2 & 0.10 & 0.3 & 0.3 & 96.223 & 83.060 & 93.024 & 70.152\\
2 & 0.10 & 0.3 & 0.5 & 96.232 & 83.573 & 93.170 & 71.827\\
2 & 0.10 & 0.5 & 0.3 & 96.232 & 83.573 & 93.190 & 72.132\\
5 & 0.01 & 0.3 & 0.3 & 96.218 & 82.752 & 92.824 & 67.817\\
5 & 0.01 & 0.3 & 0.5 & 96.223 & 83.162 & 92.991 & 69.695\\
5 & 0.01 & 0.5 & 0.3 & 96.225 & 83.265 & 93.020 & 70.102\\
5 & 0.05 & 0.3 & 0.3 & 96.220 & 82.957 & 92.941 & 69.086\\
5 & 0.05 & 0.3 & 0.5 & 96.230 & 83.470 & 93.128 & 71.371\\
5 & 0.05 & 0.5 & 0.3 & 96.230 & 83.470 & 93.111 & 71.269\\
5 & 0.10 & 0.3 & 0.3 & 96.225 & 83.162 & 93.041 & 70.406\\
5 & 0.10 & 0.3 & 0.5 & 96.237 & 83.778 & 93.174 & 71.980\\
{\bf 5} & {\bf 0.10} & {\bf 0.5} & {\bf 0.3} & {\bf 96.237} & {\bf 83.778} &
{\bf 93.195} & {\bf 72.183}\\
10 & 0.01 & 0.3 & 0.3 & 96.225 & 82.957 & 92.812 & 67.766\\
10 & 0.01 & 0.3 & 0.5 & 96.227 & 83.368 & 92.957 & 69.442\\
10 & 0.01 & 0.5 & 0.3 & 96.230 & 83.470 & 92.999 & 69.949\\
10 & 0.05 & 0.3 & 0.3 & 96.220 & 82.957 & 92.920 & 68.934\\
10 & 0.05 & 0.3 & 0.5 & 96.225 & 83.265 & 93.107 & 71.269\\
10 & 0.05 & 0.5 & 0.3 & 96.225 & 83.265 & 93.120 & 71.472\\
10 & 0.10 & 0.3 & 0.3 & 96.223 & 83.162 & 93.032 & 70.305\\
10 & 0.10 & 0.3 & 0.5 & 96.232 & 83.573 & 93.161 & 71.929\\
10 & 0.10 & 0.5 & 0.3 & 96.232 & 83.573 & 93.174 & 72.030\\
\bottomrule
\end{tabular}
\end{center}
\caption{%
The performance of the \emph{Unknown} model (described in the text) on the
in-domain and out-of-domain validation sets.
The column ``threshold'' indicates the maximum number of times a word can
appear in the training corpus and still be used to train the unknown word
model and $\theta_1$, $\theta_2$ and $\theta_3$ are the parameters that
control the interpolation of the unknown word suffix model.
The performance is measured in percentage of validation examples correctly
classified.
\label{tab:unknown}}
\end{table}

\section{Remaining Errors}

In order to interpret these final numbers, I ran the preferred model from the
previous section and saved all the mis-classifications.
On the in-domain validation set, most of the remaining errors are somewhat
subtle---for example, confusion between \code{VBD} (the past tense) and
\code{VBN} (the past participle)---but a major class of confusions occur when
a word sits at the beginning of the sentence.
Many capitalized words are classified as unknown because most words are never
capitalized in the training set.
When they are classified as such, the unknown word model steps in and
classifies the word as a proper noun with high confidence.
The best technique that I can think of for fixing this problem would be to
consider the local trigram context when computing the emission probability
(see \fig{pgm2}).
Another simpler technique would be to always lowercase the first letter of the
sentence when training and testing.

The errors in the out-of-domain validation set are much more interesting.
One major difference with this dataset is that there are many more unknown
words.
These words seem to fall into a few main categories:
\begin{itemize}
\item{modern words (``FAQ'', ``website'', ``9/11'', \emph{etc.}),}
\item{alternate/foreign spellings (``neighbourhood'' instead of
``neighborhood''),}
\item{short forms or slang (``\#'' for ``number''),}
\item{unrecognized symbols (in particular double quotes and ``- -''), and}
\item{plain old typos (``pic's'' instead of ``pics'').}
\end{itemize}
The best first step for dealing with these problems would probably be to try
and determine a set of rules that can normalize the data---correct common
typos/misspellings, convert symbols to a standard form or the equivalent word.
These rules would probably need to be developed by studying the errors made by
algorithms on a validation set like this one so in practice it would probably
be extremely hard to lock down even a substantial fraction of the false
classifications.
Another related source of errors are inconsistencies in the tokenization and
tagging conventions.
In the training data, hyphenated words are split into three or more tokens
whereas in the out-of-domain validation data, this is not always the case.

In the end, despite all these problems, the accuracy of my preferred
model---at 96.24\% on the in-domain validation set, 93.20\% on the
out-of-domain validation set and 91.34\% on the out-of-domain test set---seems
reasonable for the purposes of this assignment.
My intuition is that the emission model (not the transition model) is the most
brittle component of this model because the empirical distributions are
necessarily sparse.
With this in mind, the best way to make progress and improve the predictive
power of this model would probably be to develop a more sophisticated emission
model that considers the local (or possibly even global) context.
That being said, it would be very interesting to try and infer the syntactic
structure of the sentence simultaneously with the part-of-speech tags
conditioned directly on the words.
Also, this model (and a more complicated syntactic model) would clearly break
down in a real world situation where the ``test data'' don't correctly follow
the grammatical rules of English.

\begin{figure}[htbp]
\begin{center}
    \includegraphics{pgm2.pdf}
\end{center}
\caption{%
A generalization of \fig{pgm} that could dealing with mis-classifications of
capitalized words at the beginning of the sentence.
\figlabel{pgm2}}
\end{figure}

\begin{thebibliography}{}\raggedright

\bibitem[Brants(2000)]{tnt} Thorsten Brants (2000)
\textbf{TnT---A Statistical Part-of-Speech Tagger}.
In \emph{Proceedings of the Sixth Applied Natural Language Processing
Conference}. Seattle, WA.

\end{thebibliography}

\end{document}
